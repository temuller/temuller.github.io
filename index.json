[{"authors":["admin"],"categories":null,"content":"I am an astrophysicist working on Supernovae doing cosmology with Type Ia SNe and studying the physical parameters of Type II SNe. Machine learning and artificial intelligence are some of the methods I use for my research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://temuller.github.io/author/tomas-e.-muller-bravo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tomas-e.-muller-bravo/","section":"authors","summary":"I am an astrophysicist working on Supernovae doing cosmology with Type Ia SNe and studying the physical parameters of Type II SNe. Machine learning and artificial intelligence are some of the methods I use for my research.","tags":null,"title":"Tomás E. Müller Bravo","type":"authors"},{"authors":null,"categories":null,"content":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import scipy import lmfit import emcee #import pymc3 import pystan import iminuit from iminuit.util import describe, make_func_code from keras.layers import Dense, Activation from keras.models import Sequential from multiprocessing import Pool from chainconsumer import ChainConsumer sns.set(context='talk', style='white') %config InlineBackend.figure_format = 'retina' np.random.seed(32)  This example was taken from the emcee documentation.\nTo avoid correlation between parameters in this case, one would need to shift the x-axis by the mean value, but I will ommit that in here for simplicity.\n# Choose the \u0026quot;true\u0026quot; parameters. m_true = -0.9594 b_true = 4.294 f_true = 0.534 # Generate some synthetic data from the model. N = 50 x = np.sort(10 * np.random.rand(N)) yerr = 0.1 + 0.5 * np.random.rand(N) y = m_true * x + b_true y += np.abs(f_true * y) * np.random.randn(N) y += yerr * np.random.randn(N) plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) x0 = np.linspace(0, 10, 500) plt.plot(x0, m_true * x0 + b_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show()  scipy - minimize def log_likelihood(theta, x, y, yerr): m, b = theta model = m*x + b sigma2 = yerr**2 return np.sum((y - model)**2 / sigma2) p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) results = scipy.optimize.minimize(log_likelihood, p0, args=(x, y, yerr)) m_pred, b_pred = results.x y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} (b_true = {b_true})')  m = -0.8139 (m_true = -0.9594) b = 3.792 (b_true = 4.294)  scipy - curve_fit def function(x, m, b): model = m*x + b return model p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) pfit, pcov = scipy.optimize.curve_fit(function, x, y, p0=p0, sigma=yerr) m_pred, b_pred = pfit m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1]) y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8139 +/- 0.0647 (m_true = -0.9594) b = 3.792 +/- 0.344 (b_true = 4.294)  scipy - leastsq def residual_function(theta, x, y, yerr): m, b = theta model = m*x + b return (model - y)/yerr p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) pfit, pcov, infodict, errmsg, success = scipy.optimize.leastsq(residual_function, p0, args=(x, y, yerr), full_output=1) m_pred, b_pred = pfit try: m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1]) except: m_std = b_std = np.inf y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8139 +/- 0.0127 (m_true = -0.9594) b = 3.792 +/- 0.068 (b_true = 4.294)  lmfit def residual_function(params, x, y, yerr): m, b = params['m'].value, params['b'].value model = m*x + b return ((model - y)/yerr)**2 p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) params = lmfit.Parameters() params.add('m', value=p0[0]) params.add('b', value=p0[1]) results = lmfit.minimizer.minimize(residual_function, params, args=(x, y, yerr) , method='lbfgsb') m_pred, b_pred = results.params['m'].value, results.params['b'].value m_std, b_std = results.params['m'].stderr, results.params['b'].stderr if m_std is None and b_std is None: m_std = b_std = np.inf y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8605 +/- inf (m_true = -0.9594) b = 4.010 +/- inf (b_true = 4.294)  iminuit from iminuit import Minuit def line(m, b, x, y): return m*x + b def residual_function(m, b): #b, m = theta model = line(m, b, x, y) return np.sum(((model - y)/yerr)**2) minu = Minuit(residual_function) minu.migrad() # run optimiser minu.hesse() # run covariance estimator minu.minos() # run minos estimator m_pred, b_pred = minu.values.values() m_std, b_std = minu.errors.values() m_std_max, b_std_max, m_std_min, b_std_min = minu.merrors.values() y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print('Hesse') print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})') print('Minos') print(f'm = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- ({b_std_min:.3f}, {b_std_max:.3f}) (b_true = {b_true})') minu.draw_mncontour('m', 'b', nsigma=3)  Hesse m = -0.8139 +/- 0.0127 (m_true = -0.9594) b = 3.792 +/- 0.068 (b_true = 4.294) Minos m = -0.8139 +/- (-0.0127, 0.0127) (m_true = -0.9594) b = 3.792 +/- (-0.068, 0.068) (b_true = 4.294) \u0026lt;matplotlib.contour.ContourSet at 0x7fd0b817a208\u0026gt;    MCMC inference There are a couple of packages for plotting the samples with these methods. One is corner, which is well known by most people I would say, and the other one, which I actually prefer and use here, is chainconsumer.\n emcee def log_like(theta, x, y, yerr): m, b = theta model = m*x + b sigma2 = yerr ** 2 return -0.5 * np.sum((y - model)**2/sigma2 + np.log(sigma2)) def log_prior(theta): m, b = theta if -5.0 \u0026lt; m \u0026lt; 0.5 and 0.0 \u0026lt; b \u0026lt; 10.0: return 0.0 return -np.inf def log_probability(theta, x, y, yerr): lp = log_prior(theta) if not np.isfinite(lp): return -np.inf return lp + log_like(theta, x, y, yerr) pos = np.array([m_true, b_true]) + 1e-4*np.random.randn(32, 2) nwalkers, ndim = pos.shape with Pool() as pool: sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(x, y, yerr), pool=pool) sampler.run_mcmc(pos, 4000, progress=True) samples = sampler.chain[:, 1000:, :].reshape((-1, ndim))  100%|██████████| 4000/4000 [00:13\u0026lt;00:00, 288.01it/s]  cc = ChainConsumer() cc.add_chain(samples, parameters=['m', 'b']) # plot chains fig = cc.plotter.plot_walks(truth={\u0026quot;m\u0026quot;: m_true, \u0026quot;b\u0026quot;: b_true}, convolve=100) plt.show() # plot contours fig = cc.plotter.plot(figsize=float(ndim), truth={\u0026quot;m\u0026quot;: m_true, \u0026quot;b\u0026quot;: b_true}) plt.show()  m_mcmc = np.percentile(samples[:, 0], [16, 50, 84]) b_mcmc = np.percentile(samples[:, 1], [16, 50, 84]) m_pred, b_pred = m_mcmc[1], b_mcmc[1] m_std_min, m_std_max = np.diff(m_mcmc) b_std_min, b_std_max = np.diff(b_mcmc) y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})') print(f'b = {b_pred:.4f} +/- ({b_std_min:.4f}, {b_std_max:.4f}) (b_true = {b_true})')  m = -0.8131 +/- (0.0127, 0.0124) (m_true = -0.9594) b = 3.7896 +/- (0.0679, 0.0674) (b_true = 4.294)  pystan model = \u0026quot;\u0026quot;\u0026quot; data { int\u0026lt;lower=0\u0026gt; N; vector[N] x; vector[N] y; } parameters { real m; real b; real\u0026lt;lower=0\u0026gt; sigma; } model { y ~ normal(b + m*x, sigma); } \u0026quot;\u0026quot;\u0026quot; data = {'N': len(x), 'x': x, 'y': y} # Compile the model sm = pystan.StanModel(model_code=model) # Train the model and generate samples fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)  INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1defb22038d84b88c73c6495096e3e42 NOW.  summary_dict = fit.summary() df = pd.DataFrame(summary_dict['summary'], columns=summary_dict['summary_colnames'], index=summary_dict['summary_rownames']) m_pred, b_pred = df['mean']['m'], df['mean']['b'] m_std, b_std = df['sd']['m'], df['sd']['b'] # Extracting traces m_trace = fit['m'] b_trace = fit['b'] sigma = fit['sigma'] lp = fit['lp__'] y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.7944 +/- 0.0828 (m_true = -0.9594) b = 3.595 +/- 0.492 (b_true = 4.294)  cc = ChainConsumer() cc.add_chain(np.array([m_trace, b_trace]).T, parameters=['m', 'b']) # plot chains fig = cc.plotter.plot_walks(truth={\u0026quot;m\u0026quot;: m_true, \u0026quot;b\u0026quot;: b_true}, convolve=100) plt.show() # plot contours fig = cc.plotter.plot(figsize=float(ndim), truth={\u0026quot;m\u0026quot;: m_true, \u0026quot;b\u0026quot;: b_true}) plt.show()  pymc3 basic_model = pymc3.Model() with basic_model: p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) # Priors for unknown model parameters m = pymc3.Normal('m', mu=p0[0], sigma=2) b = pymc3.Normal('b', mu=p0[1], sigma=5) sigma = pymc3.HalfNormal('sigma', sigma=1) # Expected value of outcome model =m*x + b # Likelihood (sampling distribution) of observations Y_obs = pymc3.Normal('Y_obs', mu=model, sigma=sigma, observed=y) map_estimate = pymc3.find_MAP(model=basic_model) with basic_model: # instantiate sampler step = pymc3.Slice() # draw 5000 posterior samples trace = pymc3.sample(5000, step=step) pymc3.traceplot(trace); pymc3.summary(trace).round(2)  Other packages There are other packages for performing MCMC inference like: Pyro/NumPyro, mici, TensorFlow Probability and Sampyl.\n  Neural Networks regression The NN will fit the data without a given model. A proper fit would require training sets, testing sets and cross validation, but here only the most basic implementation is shown.\n# NN model model = Sequential() model.add(Dense(32, activation = 'relu')) model.add(Dense(units = 32, activation = 'relu')) model.add(Dense(units = 32, activation = 'relu')) model.add(Dense(units = 1)) # Compiling the ANN model.compile(optimizer = 'adam', loss = 'mean_squared_error') model.fit(x[:, None], y, batch_size = 10, epochs = 100, verbose=0) y_pred = model.predict(x[:, None]) y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) #plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.plot(x[:, None], y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show()   ","date":1594944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594944000,"objectID":"a623c58495fdc38833593e2fcd0981b0","permalink":"https://temuller.github.io/post/fitting_data/","publishdate":"2020-07-17T00:00:00Z","relpermalink":"/post/fitting_data/","section":"post","summary":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import scipy import lmfit import emcee #import pymc3 import pystan import iminuit from iminuit.util import describe, make_func_code from keras.","tags":["data science","hugo"],"title":"Fitting Data","type":"post"},{"authors":null,"categories":null,"content":"This is a code I have developed as part of my PhD to fit Type Ia Supernovae light curves in a data-driven way using Gaussian Process. The post analysis is done with Non-negative Matrix Factorization, a machine learning approach to extract features/parameters from the rest-frame light curves to do cosmology.\nPress the tag below to see more examples\n","date":1594940400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594940400,"objectID":"52dd488302b75c024bb4d2e0744506d6","permalink":"https://temuller.github.io/project/piscola/","publishdate":"2020-07-17T00:00:00+01:00","relpermalink":"/project/piscola/","section":"project","summary":"Type Ia Supernova light curve fitting code","tags":["Machine Learning"],"title":"PISCOLA","type":"project"},{"authors":["Tomás E. Müller Bravo","Claudia P Gutiérrez","Mark Sullivan","Anders Jerkstrand ..."],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"d5065d10c827b5b76768890809bcbd04","permalink":"https://temuller.github.io/publication/muller_bravo_2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/muller_bravo_2020/","section":"publication","summary":"Low-luminosity type II supernovae (LL SNe II) make up the low explosion energy end of core-collapse SNe, but their study and physical understanding remain limited. We present SN\\,2016aqf, a LL SN II with extensive spectral and photometric coverage. We measure a $V$-band peak magnitude of $-14.58$\\,mag, a plateau duration of $\\sim$100\\,days, and an inferred $^{56}$Ni mass of $0.008 \\pm 0.002$\\,\\msun. The peak bolometric luminosity, L$_{\\rm bol} \\approx 10^{41.4}$\\,erg\\,s$^{-1}$, and its spectral evolution is typical of other SNe in the class. Using our late-time spectra, we measure the [\\ion{O}{i}] $\\lambda\\lambda6300, 6364$ lines, which we compare against SN II spectral synthesis models to constrain the progenitor zero-age main-sequence mass. We find this to be 12 $\\pm$ 3\\,\\msun. Our extensive late-time spectral coverage of the [\\ion{Fe}{ii}] $\\lambda7155$ and [\\ion{Ni}{ii}] $\\lambda7378$ lines permits a measurement of the Ni/Fe abundance ratio, a parameter sensitive to the inner progenitor structure and explosion mechanism dynamics. We measure a constant abundance ratio evolution of $0.081^{+0.009}_{-0.010}$, and argue that the best epochs to measure the ratio are at $\\sim$200 -- 300\\,days after explosion. We place this measurement in the context of a large sample of SNe II and compare against various physical, light-curve and spectral parameters, in search of trends which might allow indirect ways of constraining this ratio. We do not find correlations predicted by theoretical models; however, this may be the result of the exact choice of parameters and explosion mechanism in the models, the simplicity of them and/or primordial contamination in the measured abundance ratio.","tags":null,"title":"The low-luminosity type II SN 2016aqf: A well-monitored spectral evolution of the Ni/Fe abundance ratio","type":"publication"},{"authors":[" Pursiainen, M.","Gutiérrez, C. P.","Wiseman, P.","... **T. Müller** ..."],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"86ee6aab82989c9c124cfcd7a0304003","permalink":"https://temuller.github.io/publication/pursiainen_2020/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/pursiainen_2020/","section":"publication","summary":"We present an analysis of DES17X1boj and DES16E2bjy, two peculiar transients discovered by the Dark Energy Survey (DES). They exhibit nearly identical double-peaked light curves that reach very different maximum luminosities (M$_r$ = -15.4 and -17.9, respectively). The light-curve evolution of these events is highly atypical and has not been reported before. The transients are found in different host environments: DES17X1boj was found near the nucleus of a spiral galaxy, while DES16E2bjy is located in the outskirts of a passive red galaxy. Early photometric data are well fitted with a blackbody and the resulting moderate photospheric expansion velocities (1800 km s$^{-1}$ for DES17X1boj and 4800 km s$^{-1}$ for DES16E2bjy) suggest an explosive or eruptive origin. Additionally, a feature identified as high-velocity Ca II absorption ( v $\\sim$ 9400 km s$^{-1}$) in the near-peak spectrum of DES17X1boj may imply that it is a supernova. While similar light-curve evolution suggests a similar physical origin for these two transients, we are not able to identify or characterize the progenitors.","tags":null,"title":"The mystery of photometric twins DES17X1boj and DES16E2bjy","type":"publication"},{"authors":["Szalai, Tamás","Zsíros, Szanna","Fox, Ori D.","Pejcha, Ondřej","**Müller, Tomás**"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"03eea3aa8419b72c25f8da8f4165abb6","permalink":"https://temuller.github.io/publication/szalai_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/szalai_2019/","section":"publication","summary":"The mid-infrared (mid-IR) wavelength regime offers several advantages for following the late-time evolution of supernovae (SNe). First, the peaks of the SN spectral energy distributions shift toward longer wavelengths, following the photospheric phase. Second, mid-IR observations suffer less from effects of interstellar extinction. Third, and perhaps most important, the mid-IR traces dust formation and circumstellar interaction at late times (100 days) after the radioactive ejecta component fades. The Spitzer Space Telescope has provided substantial mid-IR observations of SNe since its launch in 2003. More than 200 SNe have been targeted, but there are even more SNe that have been observed serendipitously. Here we present the results of a comprehensive study based on archival Spitzer/IRAC images of more than 1100 SN positions; from this sample, 119 SNe of various subclasses have been detected, including 45 SNe with previously unpublished mid-IR photometry. The photometry reveals significant amounts of warm dust in some cases. We perform an in-depth analysis to constrain the origin and heating mechanism of the dust, and present the resulting statistics. ","tags":null,"title":"A Comprehensive Analysis of Spitzer Supernovae","type":"publication"},{"authors":null,"categories":null,"content":"Aura, La Serena, Chile\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e52a31e850fe377d86d7d7258281554c","permalink":"https://temuller.github.io/talk/la-serena-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talk/la-serena-2019/","section":"talk","summary":"Aura, La Serena, Chile","tags":null,"title":"Cosmology with VEILS: Building an Infrared SN Ia Hubble Diagram","type":"talk"},{"authors":null,"categories":null,"content":"European Southern Observatory, Santiago, Chile\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"37a1c867972525ebc0fafd5888e6e56e","permalink":"https://temuller.github.io/talk/eso-chile-2018/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/talk/eso-chile-2018/","section":"talk","summary":"European Southern Observatory, Santiago, Chile","tags":null,"title":"Cosmology with VEILS: Building an Infrared SN Ia Hubble Diagram","type":"talk"},{"authors":null,"categories":null,"content":"Charles University, Prague, Czech Republic\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"aea138e032bf46f46c09bd6902187c4e","permalink":"https://temuller.github.io/talk/charles-university-2018/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/talk/charles-university-2018/","section":"talk","summary":"Charles University, Prague, Czech Republic","tags":null,"title":"Type Ia Supernova Cosmology: infrared light as a new window","type":"talk"},{"authors":null,"categories":null,"content":"A 2-days Software Carpentry Workshop to teach PGR students at the Max Planck Institute for Plasma Physics (Greifswald) the basic of Shell, Python and Git.\nDates: Apr 16-17, 2018\n Workshop website\nPress the tag below to see more examples\n","date":1523833200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523833200,"objectID":"c6b419033f60d6c4ccd3518849e3c425","permalink":"https://temuller.github.io/meeting/scw_max_planck_2018/","publishdate":"2018-04-16T00:00:00+01:00","relpermalink":"/meeting/scw_max_planck_2018/","section":"meeting","summary":"SCW at the Max Planck Institute for Plasma Physics ([Workshop website](https://temuller.github.io/2018-04-16-maxplanck/))","tags":["Software"],"title":"Software Carpentry Workshop","type":"meeting"},{"authors":null,"categories":null,"content":"A 2-days Software Carpentry Workshop to teach PGR students at the University of Southampton the basic of Shell, Python and Git.\nDates: Apr 12-13, 2018\n Workshop website\nPress the tag below to see more examples\n","date":1523487600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523487600,"objectID":"6f568912ed161f6eceef9c43711749bc","permalink":"https://temuller.github.io/meeting/scw_soton_2018/","publishdate":"2018-04-12T00:00:00+01:00","relpermalink":"/meeting/scw_soton_2018/","section":"meeting","summary":"SCW at the University of Southampton ([Workshop website](https://southampton-rsg.github.io/2018-04-12-southampton-swc/))","tags":["Software"],"title":"Software Carpentry Workshop","type":"meeting"},{"authors":["Müller, Tomás","Prieto, José L.","Pejcha, Ondřej","Clocchiatti, Alejandro"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"a0f15357430fe14e74d7be22c4154154","permalink":"https://temuller.github.io/publication/muller_2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/muller_2017/","section":"publication","summary":"Core-collapse supernova explosions expose the structure and environment of massive stars at the moment of their death. We use the global fitting technique of Pejcha \u0026 Prieto (2015a,b) to estimate a set of physical parameters of 19 normal Type II SNe, such as their distance moduli, reddenings, $^{56}$Ni masses M$_{\\rm Ni}$, and explosion energies E$_{\\rm exp}$ from multicolor light curves and photospheric velocity curves. We confirm and characterize known correlations between MNi and bolometric luminosity at 50 days after the explosion, and between MNi and Eexp. We pay special attention to the observed distribution of M$_{\\rm Ni}$ coming from a joint sample of 38 Type II SNe, which can be described as a skewed-Gaussian-like distribution between 0.005 M$_{\\odot}$ and 0.280 M$_{\\odot}$, with a median of 0.031 M$_{\\odot}$, mean of 0.046 M$_{\\odot}$, standard deviation of 0.048 M$_{\\odot}$ and skewness of 3.050. We use two-sample Kolmogorov-Smirnov test and two-sample Anderson-Darling test to compare the observed distribution of MNi to results from theoretical hydrodynamical codes of core-collapse explosions with the neutrino mechanism presented in the literature. Our results show that the theoretical distributions obtained from the codes tested in this work, KEPLER and Prometheus Hot Bubble, are compatible with the observations irrespective of different pre-supernova calibrations and different maximum mass of the progenitors. ","tags":null,"title":"The Nickel Mass Distribution of Normal Type II Supernovae","type":"publication"},{"authors":["Kochanek, C. S.","Fraser, M.","Adams, S. M.","Sukhbold, T.","Prieto, J. L.","**Müller T.** ..."],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"0a944dcbfbbe50ea208b201714198502","permalink":"https://temuller.github.io/publication/kochaneck_2017/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/kochaneck_2017/","section":"publication","summary":"We identify a pre-explosion counterpart to the nearby Type IIP supernova ASASSN-16fq (SN 2016cok) in archival Hubble Space Telescope (HST) data. The source appears to be a blend of several stars that prevents obtaining accurate photometry. However, with reasonable assumptions about the stellar temperature and extinction, the progenitor almost certainly had an initial mass M$","tags":null,"title":"Supernova progenitors, their variability and the Type IIP Supernova ASASSN-16fq in M66 ","type":"publication"}]