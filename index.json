[{"authors":["admin"],"categories":null,"content":"I am an astrophysicist working on Supernovae doing cosmology with Type Ia SNe and studying the physical parameters of Type II SNe. Machine learning and artificial intelligence are some of the methods I use for my research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://temuller.github.io/author/tomas-e.-muller-bravo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tomas-e.-muller-bravo/","section":"authors","summary":"I am an astrophysicist working on Supernovae doing cosmology with Type Ia SNe and studying the physical parameters of Type II SNe. Machine learning and artificial intelligence are some of the methods I use for my research.","tags":null,"title":"Tomás E. Müller Bravo","type":"authors"},{"authors":null,"categories":null,"content":"Every year SEPnet gives the opportunity to a group of postgraduate researchers (PGRs) to organise a two-days student-led conference hosted in Southampton. I first came to know about this when I applied for a talk at the Astronomy conference from 2019 ( From Infinity to Zero: the history of the Universe in redshift). These type of conferences are perfect for early career researchers as it is organised by student, for students, although a few academic speakers are also invited. You do not feel the same pressure as with the big international conferences where you find all the \u0026ldquo;big names\u0026rdquo; from your research field. In addition, you also get to know lots of people in the same career stages as you and share experiences.\nAfter the conference I attended was over, I got very interested in organising one of my own. Therefore, I talked to Elizabeth Swann, the lead organiser of the 2019 meeting, to ask her for advise (this was really helpful!). I got very excited with the idea, so I decided to ask around in my Astronomy departement (University of Southampton) for fellow PGRs interested in organising a conference together and started writing a proposal. It happened that, at the same time, a group of PGRs from the University of Hertfordshire were writing one of their own. They kindly suggested to work together instead of compiting, so we started collaborating on a single proposal (afterall, isn\u0026rsquo;t this what research is all about?).\nChoosing the topic of the conference was relatively easy. As many big telescopes and surveys are coming in the near future, we thought it might be a good idea to focus on big data and machine learning, thus, the title of the conference: The Big Data Era in Astronomy. Finding academic speakers wasn\u0026rsquo;t too hard either. As several of us in the organising committee work on different fields, we quickly came up with a list of candidates. Cristobel Soares-Smith was in charge of most of the logistics (including funding) and she was also very helpful with her advice, so we mainly had to focus on the structure, science and social events (for example, a conference dinner) of the meeting. Everything was going great as the start of the conference was approaching, however, everything changed when the COVID-19 pandemic struck.\nMany things were quite uncertain at that time. We didn\u0026rsquo;t know how long and how much this pandemic would affect everyone, so we had to postpone the conference. As time started passing by, we grew impasient. We didn\u0026rsquo;t know if we were going to have the opportunity to host the meeting or if we would have to cancel it. Eventually, we decided to do what many other conferences, schools and workshops were doing, choose a new date and go virtual!\nThis was full of challenges. We didn\u0026rsquo;t have to worry about funding, conference dinner and other things, but we did have to think about the proper platforms to host our virtual conference on. Thankfully, all Universities in SEPnet have access to Microsoft Teams, thus, we chose it as our platform for hosting the talks. In addition, Slack is widely used in academic environments as it is perfect for asynchronous discussions, questions, announcements, etc. We also decided to use Slido for the questions at the end of each talk, which was quite new to all of us (I got to know about Slido during an ESO conference in June).\nUnfortunately, as things were still uncertain close to the new date of the conference in September, many of the students were unable to attend. Therefore, we had to shorten the length of the conference from two to one day, and cut the number of sessions by half. However, the conference turned out better than expected. From the feedback of the participants we learnt that the length of the conference (including the length of talks, breaks, etc.) was of their liking. Furthermore, the platforms used (Microsoft Teams, Slack, Slido) were really useful and the attendees found that the conference was worth assisting. Nonetheless, most of them, given the opportunity to choose, would prefer an in-person meeting instead of a virtual one.\nNetworking and interacting with other people online is not as easy as in person. It is hard to have spontaneous and/or informal encounters. So, from my point of view, hosting a virtual conference has many advantages (for example, it is cheaper, easier to organise, no need to travel, etc.) and is a great option given the current circumstances, but it would always lack the face-to-face interaction which is a big part of these events. However, it is clear that the pandemic has opened a new window to future conferences as virtual meeting are becoming more common and reliable than in the past.\n","date":1600214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600214400,"objectID":"e8ab4073970685b0e797b75c1b82b97a","permalink":"https://temuller.github.io/post/student_led_conference/","publishdate":"2020-09-16T00:00:00Z","relpermalink":"/post/student_led_conference/","section":"post","summary":"Every year SEPnet gives the opportunity to a group of postgraduate researchers (PGRs) to organise a two-days student-led conference hosted in Southampton. I first came to know about this when I applied for a talk at the Astronomy conference from 2019 ( From Infinity to Zero: the history of the Universe in redshift).","tags":["astronomy","supernova"],"title":" Organising my first conference","type":"post"},{"authors":null,"categories":null,"content":"Every year a student-led conference that lasts for 2 and a half days is hosted at the University of Southampton which PGR students from SEPnet can attend to dig into the world of research conferences. I was the lead organizer in a committee of PGR students from the University of Southampton and University of Hertfordshire. Because of the COVID-19 pandemic we had to host a one-day conference online.\nDates: Sep 8, 2020\n Conference website\nPress the tag below to see more examples\n","date":1599519600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599519600,"objectID":"23b6417637a2d70cae2b1238733c4c36","permalink":"https://temuller.github.io/meeting/student_led_conference_2020/","publishdate":"2020-09-08T00:00:00+01:00","relpermalink":"/meeting/student_led_conference_2020/","section":"meeting","summary":"Student-led conference hosted at the University of Southampton ([Conference website](https://sites.google.com/view/the-big-data-era-in-astronomy/home))","tags":["Astro"],"title":"The Big Data Era in Astronomy","type":"meeting"},{"authors":null,"categories":null,"content":"In this notebook I show some basic implementation of different Python packages for data fitting. The idea is to learn the different options there are out there so the reader can then study them in more detail if needed. This notebook can be opened on google colab or binder. If for some reason there is a package missing, you will need to manually install it by running !pip install \u0026lt;package\u0026gt; in a cell.\nTo open this notebook on google colab, click in the following icon: \nTo open this notebook on binder, click in the following icon: \nimport matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import scipy import lmfit import emcee #import pymc3 # borken installation? import pystan import iminuit from iminuit.util import describe, make_func_code from keras.layers import Dense, Activation from keras.models import Sequential from multiprocessing import Pool import corner sns.set(context='talk', style='white') %config InlineBackend.figure_format = 'retina' np.random.seed(32)  This example, which represents data taken from a line, was taken from the emcee documentation.\nTo avoid correlation between parameters in this case, one would need to shift the x-axis by the mean value, but I will ommit that in here for simplicity. I will only show how to implement the different packages.\n# Choose the \u0026quot;true\u0026quot; parameters. m_true = -0.9594 b_true = 4.294 f_true = 0.534 # Generate some synthetic data from the model. N = 50 x = np.sort(10 * np.random.rand(N)) yerr = 0.1 + 0.5 * np.random.rand(N) y = m_true * x + b_true y += np.abs(f_true * y) * np.random.randn(N) y += yerr * np.random.randn(N) plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) x0 = np.linspace(0, 10, 500) plt.plot(x0, m_true * x0 + b_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show()  scipy - minimize def log_likelihood(theta, x, y, yerr): m, b = theta model = m*x + b sigma2 = yerr**2 return np.sum((y - model)**2 / sigma2) p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) results = scipy.optimize.minimize(log_likelihood, p0, args=(x, y, yerr)) m_pred, b_pred = results.x y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} (b_true = {b_true})')  m = -0.8139 (m_true = -0.9594) b = 3.792 (b_true = 4.294)  scipy - curve_fit def function(x, m, b): model = m*x + b return model p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) pfit, pcov = scipy.optimize.curve_fit(function, x, y, p0=p0, sigma=yerr) m_pred, b_pred = pfit m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1]) y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8139 +/- 0.0647 (m_true = -0.9594) b = 3.792 +/- 0.344 (b_true = 4.294)  scipy - leastsq def residual_function(theta, x, y, yerr): m, b = theta model = m*x + b return (model - y)/yerr p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) pfit, pcov, infodict, errmsg, success = scipy.optimize.leastsq(residual_function, p0, args=(x, y, yerr), full_output=1) m_pred, b_pred = pfit try: m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1]) except: m_std = b_std = np.inf y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8139 +/- 0.0127 (m_true = -0.9594) b = 3.792 +/- 0.068 (b_true = 4.294)  lmfit def residual_function(params, x, y, yerr): m, b = params['m'].value, params['b'].value model = m*x + b return ((model - y)/yerr)**2 p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) params = lmfit.Parameters() params.add('m', value=p0[0]) params.add('b', value=p0[1]) results = lmfit.minimizer.minimize(residual_function, params, args=(x, y, yerr) , method='lbfgsb') m_pred, b_pred = results.params['m'].value, results.params['b'].value m_std, b_std = results.params['m'].stderr, results.params['b'].stderr if m_std is None and b_std is None: m_std = b_std = np.inf y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.8605 +/- inf (m_true = -0.9594) b = 4.010 +/- inf (b_true = 4.294)  iminuit from iminuit import Minuit def line(m, b, x, y): return m*x + b def residual_function(m, b): #b, m = theta model = line(m, b, x, y) return np.sum(((model - y)/yerr)**2) minu = Minuit(residual_function) minu.migrad() # run optimiser minu.hesse() # run covariance estimator minu.minos() # run minos estimator m_pred, b_pred = minu.values.values() m_std, b_std = minu.errors.values() m_std_max, b_std_max, m_std_min, b_std_min = minu.merrors.values() y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print('Hesse') print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})') print('Minos') print(f'm = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- ({b_std_min:.3f}, {b_std_max:.3f}) (b_true = {b_true})') minu.draw_mncontour('m', 'b', nsigma=3)  /home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m does not have initial value. Assume 0. # This is added back by InteractiveShellApp.init_path() /home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m is floating but does not have initial step size. Assume 1. # This is added back by InteractiveShellApp.init_path() /home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b does not have initial value. Assume 0. # This is added back by InteractiveShellApp.init_path() /home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b is floating but does not have initial step size. Assume 1. # This is added back by InteractiveShellApp.init_path() /home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: errordef is not given. Default to 1. # This is added back by InteractiveShellApp.init_path()  Hesse m = -0.8139 +/- 0.0127 (m_true = -0.9594) b = 3.792 +/- 0.068 (b_true = 4.294) Minos m = -0.8139 +/- (-0.0127, 0.0127) (m_true = -0.9594) b = 3.792 +/- (-0.068, 0.068) (b_true = 4.294) \u0026lt;matplotlib.contour.ContourSet at 0x7fd0b817a208\u0026gt;    MCMC inference There are a couple of packages for plotting the samples with these methods. One is corner, which is well known by most people I would say, and the other one, which I actually prefer and use here, is chainconsumer.\n emcee def log_like(theta, x, y, yerr): m, b = theta model = m*x + b sigma2 = yerr ** 2 return -0.5 * np.sum((y - model)**2/sigma2 + np.log(sigma2)) def log_prior(theta): m, b = theta if -5.0 \u0026lt; m \u0026lt; 0.5 and 0.0 \u0026lt; b \u0026lt; 10.0: return 0.0 return -np.inf def log_probability(theta, x, y, yerr): lp = log_prior(theta) if not np.isfinite(lp): return -np.inf return lp + log_like(theta, x, y, yerr) pos = np.array([m_true, b_true]) + 1e-4*np.random.randn(32, 2) nwalkers, ndim = pos.shape with Pool() as pool: sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(x, y, yerr), pool=pool) sampler.run_mcmc(pos, 4000, progress=True) samples = sampler.chain[:, 1000:, :].reshape((-1, ndim))  100%|██████████| 4000/4000 [00:54\u0026lt;00:00, 73.10it/s]  fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True) labels = [\u0026quot;m\u0026quot;, \u0026quot;b\u0026quot;] for i in range(ndim): ax = axes[i] ax.plot(samples[:, i], \u0026quot;k\u0026quot;, alpha=0.6) ax.set_xlim(0, len(samples)) ax.set_ylabel(labels[i]) ax.yaxis.set_label_coords(-0.1, 0.5) axes[-1].set_xlabel(\u0026quot;step number\u0026quot;); fig = corner.corner( samples, labels=labels, truths=[m_true, b_true] );  m_mcmc = np.percentile(samples[:, 0], [16, 50, 84]) b_mcmc = np.percentile(samples[:, 1], [16, 50, 84]) m_pred, b_pred = m_mcmc[1], b_mcmc[1] m_std_min, m_std_max = np.diff(m_mcmc) b_std_min, b_std_max = np.diff(b_mcmc) y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})') print(f'b = {b_pred:.4f} +/- ({b_std_min:.4f}, {b_std_max:.4f}) (b_true = {b_true})')  m = -0.8131 +/- (0.0127, 0.0124) (m_true = -0.9594) b = 3.7896 +/- (0.0679, 0.0674) (b_true = 4.294)  pystan model = \u0026quot;\u0026quot;\u0026quot; data { int\u0026lt;lower=0\u0026gt; N; vector[N] x; vector[N] y; } parameters { real m; real b; real\u0026lt;lower=0\u0026gt; sigma; } model { y ~ normal(b + m*x, sigma); } \u0026quot;\u0026quot;\u0026quot; data = {'N': len(x), 'x': x, 'y': y} # Compile the model sm = pystan.StanModel(model_code=model) # Train the model and generate samples fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)  INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1defb22038d84b88c73c6495096e3e42 NOW.  summary_dict = fit.summary() df = pd.DataFrame(summary_dict['summary'], columns=summary_dict['summary_colnames'], index=summary_dict['summary_rownames']) m_pred, b_pred = df['mean']['m'], df['mean']['b'] m_std, b_std = df['sd']['m'], df['sd']['b'] # Extracting traces m_trace = fit['m'] b_trace = fit['b'] sigma = fit['sigma'] lp = fit['lp__'] y_pred = m_pred*x0 + b_pred y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show() print(f'm = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})') print(f'b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})')  m = -0.7944 +/- 0.0828 (m_true = -0.9594) b = 3.595 +/- 0.492 (b_true = 4.294)  samples = np.array([m_trace, b_trace]).T fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True) labels = [\u0026quot;m\u0026quot;, \u0026quot;b\u0026quot;] for i in range(ndim): ax = axes[i] ax.plot(samples[:, i], \u0026quot;k\u0026quot;, alpha=0.6) ax.set_xlim(0, len(samples)) ax.set_ylabel(labels[i]) ax.yaxis.set_label_coords(-0.1, 0.5) axes[-1].set_xlabel(\u0026quot;step number\u0026quot;); fig = corner.corner( samples, labels=labels, truths=[m_true, b_true] );  pymc3 basic_model = pymc3.Model() with basic_model: p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2) # Priors for unknown model parameters m = pymc3.Normal('m', mu=p0[0], sigma=2) b = pymc3.Normal('b', mu=p0[1], sigma=5) sigma = pymc3.HalfNormal('sigma', sigma=1) # Expected value of outcome model =m*x + b # Likelihood (sampling distribution) of observations Y_obs = pymc3.Normal('Y_obs', mu=model, sigma=sigma, observed=y) map_estimate = pymc3.find_MAP(model=basic_model) with basic_model: # instantiate sampler step = pymc3.Slice() # draw 5000 posterior samples trace = pymc3.sample(5000, step=step) pymc3.traceplot(trace); pymc3.summary(trace).round(2)  Other packages There are other packages for performing MCMC inference like: Pyro/NumPyro, mici, TensorFlow Probability and Sampyl (I might be missing a couple though). Feel free to check those as well.\n  Artificial Neural Networks (ANN) regression The ANN will fit the data without a given model. A proper fit would require training sets, testing sets and cross validation, but here only the most basic implementation is shown. There is much more you can do with ANN.\n# NN model model = Sequential() model.add(Dense(32, activation = 'relu')) model.add(Dense(units = 32, activation = 'relu')) model.add(Dense(units = 32, activation = 'relu')) model.add(Dense(units = 1)) # Compiling the ANN model.compile(optimizer = 'adam', loss = 'mean_squared_error') model.fit(x[:, None], y, batch_size = 10, epochs = 100, verbose=0) y_pred = model.predict(x[:, None]) y_true = m_true*x0 + b_true plt.errorbar(x, y, yerr=yerr, fmt=\u0026quot;.k\u0026quot;, capsize=0) plt.plot(x0, y_true, \u0026quot;k\u0026quot;, alpha=0.3, lw=3, label=\u0026quot;truth\u0026quot;) #plt.plot(x0, y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.plot(x[:, None], y_pred, \u0026quot;:k\u0026quot;, label=\u0026quot;fit\u0026quot;) plt.legend(fontsize=14) plt.xlim(0, 10) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.show()   ","date":1594944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594944000,"objectID":"a623c58495fdc38833593e2fcd0981b0","permalink":"https://temuller.github.io/post/fitting_data/","publishdate":"2020-07-17T00:00:00Z","relpermalink":"/post/fitting_data/","section":"post","summary":"In this notebook I show some basic implementation of different Python packages for data fitting. The idea is to learn the different options there are out there so the reader can then study them in more detail if needed.","tags":["data science"],"title":"Fitting Data - Basic implementation of Python packages","type":"post"},{"authors":null,"categories":null,"content":"Python for Intelligent Supernova COsmology Light-curve Analysis (PISCOLA) is a code I have developed as part of my PhD to fit Type Ia Supernova light curves in a data-driven way using Gaussian Process. The post analysis is done with Non-negative Matrix Factorization, a machine learning approach to extract features/parameters from the rest-frame light curves in order to do cosmology.\nPress the tag below to see more examples\n","date":1594940400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594940400,"objectID":"52dd488302b75c024bb4d2e0744506d6","permalink":"https://temuller.github.io/project/piscola/","publishdate":"2020-07-17T00:00:00+01:00","relpermalink":"/project/piscola/","section":"project","summary":"Type Ia Supernova light curve fitting code","tags":["Machine Learning"],"title":"PISCOLA","type":"project"},{"authors":["Tomás E. Müller Bravo","Claudia P Gutiérrez","Mark Sullivan","Anders Jerkstrand ..."],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"d5065d10c827b5b76768890809bcbd04","permalink":"https://temuller.github.io/publication/muller_bravo_2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/muller_bravo_2020/","section":"publication","summary":"Low-luminosity type II supernovae (LL SNe II) make up the low explosion energy end of core-collapse SNe, but their study and physical understanding remain limited. We present SN\\,2016aqf, a LL SN II with extensive spectral and photometric coverage. We measure a $V$-band peak magnitude of $-14.58$\\,mag, a plateau duration of $\\sim$100\\,days, and an inferred $^{56}$Ni mass of $0.008 \\pm 0.002$\\,\\msun. The peak bolometric luminosity, L$_{\\rm bol} \\approx 10^{41.4}$\\,erg\\,s$^{-1}$, and its spectral evolution is typical of other SNe in the class. Using our late-time spectra, we measure the [\\ion{O}{i}] $\\lambda\\lambda6300, 6364$ lines, which we compare against SN II spectral synthesis models to constrain the progenitor zero-age main-sequence mass. We find this to be 12 $\\pm$ 3\\,\\msun. Our extensive late-time spectral coverage of the [\\ion{Fe}{ii}] $\\lambda7155$ and [\\ion{Ni}{ii}] $\\lambda7378$ lines permits a measurement of the Ni/Fe abundance ratio, a parameter sensitive to the inner progenitor structure and explosion mechanism dynamics. We measure a constant abundance ratio evolution of $0.081^{+0.009}_{-0.010}$, and argue that the best epochs to measure the ratio are at $\\sim$200 -- 300\\,days after explosion. We place this measurement in the context of a large sample of SNe II and compare against various physical, light-curve and spectral parameters, in search of trends which might allow indirect ways of constraining this ratio. We do not find correlations predicted by theoretical models; however, this may be the result of the exact choice of parameters and explosion mechanism in the models, the simplicity of them and/or primordial contamination in the measured abundance ratio.","tags":null,"title":"The low-luminosity type II SN 2016aqf: A well-monitored spectral evolution of the Ni/Fe abundance ratio","type":"publication"},{"authors":[" Pursiainen, M.","Gutiérrez, C. P.","Wiseman, P.","... **T. Müller** ..."],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"86ee6aab82989c9c124cfcd7a0304003","permalink":"https://temuller.github.io/publication/pursiainen_2020/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/pursiainen_2020/","section":"publication","summary":"We present an analysis of DES17X1boj and DES16E2bjy, two peculiar transients discovered by the Dark Energy Survey (DES). They exhibit nearly identical double-peaked light curves that reach very different maximum luminosities (M$_r$ = -15.4 and -17.9, respectively). The light-curve evolution of these events is highly atypical and has not been reported before. The transients are found in different host environments: DES17X1boj was found near the nucleus of a spiral galaxy, while DES16E2bjy is located in the outskirts of a passive red galaxy. Early photometric data are well fitted with a blackbody and the resulting moderate photospheric expansion velocities (1800 km s$^{-1}$ for DES17X1boj and 4800 km s$^{-1}$ for DES16E2bjy) suggest an explosive or eruptive origin. Additionally, a feature identified as high-velocity Ca II absorption ( v $\\sim$ 9400 km s$^{-1}$) in the near-peak spectrum of DES17X1boj may imply that it is a supernova. While similar light-curve evolution suggests a similar physical origin for these two transients, we are not able to identify or characterize the progenitors.","tags":null,"title":"The mystery of photometric twins DES17X1boj and DES16E2bjy","type":"publication"},{"authors":["Szalai, Tamás","Zsíros, Szanna","Fox, Ori D.","Pejcha, Ondřej","**Müller, Tomás**"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"03eea3aa8419b72c25f8da8f4165abb6","permalink":"https://temuller.github.io/publication/szalai_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/szalai_2019/","section":"publication","summary":"The mid-infrared (mid-IR) wavelength regime offers several advantages for following the late-time evolution of supernovae (SNe). First, the peaks of the SN spectral energy distributions shift toward longer wavelengths, following the photospheric phase. Second, mid-IR observations suffer less from effects of interstellar extinction. Third, and perhaps most important, the mid-IR traces dust formation and circumstellar interaction at late times (100 days) after the radioactive ejecta component fades. The Spitzer Space Telescope has provided substantial mid-IR observations of SNe since its launch in 2003. More than 200 SNe have been targeted, but there are even more SNe that have been observed serendipitously. Here we present the results of a comprehensive study based on archival Spitzer/IRAC images of more than 1100 SN positions; from this sample, 119 SNe of various subclasses have been detected, including 45 SNe with previously unpublished mid-IR photometry. The photometry reveals significant amounts of warm dust in some cases. We perform an in-depth analysis to constrain the origin and heating mechanism of the dust, and present the resulting statistics. ","tags":null,"title":"A Comprehensive Analysis of Spitzer Supernovae","type":"publication"},{"authors":null,"categories":null,"content":"See below a guest post I wrote for Astrobites. For the original one click here.\n Title: The surface abundances of Red Supergiants at core-collapse\nAuthors: Ben Davies and Luc Dessart\nFirst Author\u0026rsquo;s Institution: Astrophysics Research Institute, Liverpool John Moores University\nStatus: accepted for publication in MNRAS\nCore-Collapse supernovae (CCSNe) are explosions coming from massive stars (above 8 solar masses) when they reach the end of their life. A Type II-P Supernova (SN II-P) is a common type of CCSN which shows a \u0026ldquo;plateau\u0026rdquo; in its light curve, driven by a Hydrogen-rich envelope. Observations of the explosion site of some of these objects have shown that the progenitors of these explosions are Red Supergiant (RSG) stars. From these data several predictions can be made by comparing with stellar evolution models. One test that can be made is to determine the initial mass of the progenitor (the mass at the Main-Sequence), which can be done by comparing the luminosity of the progenitor with model predictions. Another way is to measure the mass of the Hydrogen-rich envelope by modelling the light curve and making some assumptions about the core mass. A different way is to measure spectral lines of some elements, like Oxygen, at late time (more than 120 days after the explosion, approximately), which correlates with the initial mass as shown by some models.\nUnfortunately, these methods do not agree in general, so a proper estimation of the initial mass of the progenitor of a SN II-P can not be made. Having this in mind, the authors of the article proposed a different way of estimating the initial mass by measuring the surface composition (or surface abundance) of the progenitor star at early epochs (less than 1 day approximately). There are two main reasons for this: firstly, at this stage some spectral features are easy to identify, and secondly, the surface abundance is not expected to suffer from explosive mixing at early time, which erases any link to the progenitor mass.\nTo test this, the authors make use of the MESA code to study sets of evolutionary models. The evolution across the Hertzsprung-Russell (H-R) diagram is key to understand the different processes and phases a star goes through, so the authors use MESA to evolve stars with different initial masses to study this. As it can be seen in Figure 1, less massive stars cross the H-R diagram more rapidly than more massive star, which means that more massive stars have more time to dredge up material from the inner layers into the surface (mixing the abundances) before the end of the RSG phase. Another factor that has to be taken into account is the mass loss. In general, more massive star loose more mass than the least massive ones, so their outer envelopes are thinner compared to the total size of the star, hence their surface abundances suffer from more mixing.\nFig. 1: H-R diagram for stars with different initial masses as labelled. The circles mark evenly-spaced time steps. The stars indicate the beginning of the RSG phase. More massive stars take longer to cross the H-R diagram than less massive stars.\nSince the surface abundance can not be directly measured, an indirect way of measurement was developed. The authors demonstrate that the surface abundance can be estimated by measuring the ratio between spectral lines, specifically the Carbon-to-Nitrogen ratio, a few hours after the supernova explosion. They simulate early time supernova spectra coming from stars with three different initial masses. The results of these simulated spectra are shown in Figure 2 for a 15 and 25 solar masses stars, for three epochs: 3.1, 12.0 and 24.0 hours after the explosion. One can clearly see the greater strength of Nitrogen lines in the higher mass model, and the greater strength of Carbon and Oxygen lines in the lower mass model. These same trends have been seen in the observed early-time spectra of SN II-P, however, the lack of data at these early stages is clear.\nFig. 2: Early-time SN II-P spectra for progenitors of 15 (blue) and 25 (red) solar masses. From top to bottom, spectra of 3.1, 12.0 and 24.0 hours after explosion. It can be seen the difference in the strengths of lines for the two progenitors.\nLike the other methods mentioned above, the surface abundance method suffers from several uncertainties, but no further comparison is given. Finally, the author estimate that within the next decade several early time spectra of SN II-P should be available, which would help decreasing the uncertainties for a better estimation of the initial masses of their progenitors.\n","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548633600,"objectID":"27e37ed592eadecbabf2fc9471d3e585","permalink":"https://temuller.github.io/post/astrobites/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/post/astrobites/","section":"post","summary":"See below a guest post I wrote for Astrobites. For the original one click here.\n Title: The surface abundances of Red Supergiants at core-collapse\nAuthors: Ben Davies and Luc Dessart","tags":["astronomy","supernova"],"title":" Judging a book by its cover: estimating red supergiant masses from their surface abundance","type":"post"},{"authors":null,"categories":null,"content":"Aura, La Serena, Chile\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e52a31e850fe377d86d7d7258281554c","permalink":"https://temuller.github.io/talk/la-serena-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talk/la-serena-2019/","section":"talk","summary":"Aura, La Serena, Chile","tags":null,"title":"Cosmology with VEILS: Building an Infrared SN Ia Hubble Diagram","type":"talk"},{"authors":null,"categories":null,"content":"European Southern Observatory, Santiago, Chile\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"37a1c867972525ebc0fafd5888e6e56e","permalink":"https://temuller.github.io/talk/eso-chile-2018/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/talk/eso-chile-2018/","section":"talk","summary":"European Southern Observatory, Santiago, Chile","tags":null,"title":"Cosmology with VEILS: Building an Infrared SN Ia Hubble Diagram","type":"talk"},{"authors":null,"categories":null,"content":"Charles University, Prague, Czech Republic\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"aea138e032bf46f46c09bd6902187c4e","permalink":"https://temuller.github.io/talk/charles-university-2018/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/talk/charles-university-2018/","section":"talk","summary":"Charles University, Prague, Czech Republic","tags":null,"title":"Type Ia Supernova Cosmology: infrared light as a new window","type":"talk"},{"authors":null,"categories":null,"content":"A 2-days Software Carpentry Workshop to teach PGR students at the Max Planck Institute for Plasma Physics (Greifswald) the basic of Shell, Python and Git.\nDates: Apr 16-17, 2018\n Workshop website\nPress the tag below to see more examples\n","date":1523833200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523833200,"objectID":"c6b419033f60d6c4ccd3518849e3c425","permalink":"https://temuller.github.io/meeting/scw_max_planck_2018/","publishdate":"2018-04-16T00:00:00+01:00","relpermalink":"/meeting/scw_max_planck_2018/","section":"meeting","summary":"SCW at the Max Planck Institute for Plasma Physics ([Workshop website](https://temuller.github.io/2018-04-16-maxplanck/))","tags":["Software"],"title":"Software Carpentry Workshop","type":"meeting"},{"authors":null,"categories":null,"content":"A 2-days Software Carpentry Workshop to teach PGR students at the University of Southampton the basic of Shell, Python and Git.\nDates: Apr 12-13, 2018\n Workshop website\nPress the tag below to see more examples\n","date":1523487600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523487600,"objectID":"6f568912ed161f6eceef9c43711749bc","permalink":"https://temuller.github.io/meeting/scw_soton_2018/","publishdate":"2018-04-12T00:00:00+01:00","relpermalink":"/meeting/scw_soton_2018/","section":"meeting","summary":"SCW at the University of Southampton ([Workshop website](https://southampton-rsg.github.io/2018-04-12-southampton-swc/))","tags":["Software"],"title":"Software Carpentry Workshop","type":"meeting"},{"authors":["Müller, Tomás","Prieto, José L.","Pejcha, Ondřej","Clocchiatti, Alejandro"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"a0f15357430fe14e74d7be22c4154154","permalink":"https://temuller.github.io/publication/muller_2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/muller_2017/","section":"publication","summary":"Core-collapse supernova explosions expose the structure and environment of massive stars at the moment of their death. We use the global fitting technique of Pejcha \u0026 Prieto (2015a,b) to estimate a set of physical parameters of 19 normal Type II SNe, such as their distance moduli, reddenings, $^{56}$Ni masses M$_{\\rm Ni}$, and explosion energies E$_{\\rm exp}$ from multicolor light curves and photospheric velocity curves. We confirm and characterize known correlations between MNi and bolometric luminosity at 50 days after the explosion, and between MNi and Eexp. We pay special attention to the observed distribution of M$_{\\rm Ni}$ coming from a joint sample of 38 Type II SNe, which can be described as a skewed-Gaussian-like distribution between 0.005 M$_{\\odot}$ and 0.280 M$_{\\odot}$, with a median of 0.031 M$_{\\odot}$, mean of 0.046 M$_{\\odot}$, standard deviation of 0.048 M$_{\\odot}$ and skewness of 3.050. We use two-sample Kolmogorov-Smirnov test and two-sample Anderson-Darling test to compare the observed distribution of MNi to results from theoretical hydrodynamical codes of core-collapse explosions with the neutrino mechanism presented in the literature. Our results show that the theoretical distributions obtained from the codes tested in this work, KEPLER and Prometheus Hot Bubble, are compatible with the observations irrespective of different pre-supernova calibrations and different maximum mass of the progenitors. ","tags":null,"title":"The Nickel Mass Distribution of Normal Type II Supernovae","type":"publication"},{"authors":["Kochanek, C. S.","Fraser, M.","Adams, S. M.","Sukhbold, T.","Prieto, J. L.","**Müller T.** ..."],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"0a944dcbfbbe50ea208b201714198502","permalink":"https://temuller.github.io/publication/kochaneck_2017/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/kochaneck_2017/","section":"publication","summary":"We identify a pre-explosion counterpart to the nearby Type IIP supernova ASASSN-16fq (SN 2016cok) in archival Hubble Space Telescope (HST) data. The source appears to be a blend of several stars that prevents obtaining accurate photometry. However, with reasonable assumptions about the stellar temperature and extinction, the progenitor almost certainly had an initial mass M$","tags":null,"title":"Supernova progenitors, their variability and the Type IIP Supernova ASASSN-16fq in M66 ","type":"publication"}]