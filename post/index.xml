<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Tomás E. Müller Bravo</title>
    <link>https://temuller.github.io/post/</link>
      <atom:link href="https://temuller.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><copyright>© Tomás Müller `2020`</copyright><lastBuildDate>Fri, 17 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://temuller.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://temuller.github.io/post/</link>
    </image>
    
    <item>
      <title>Fitting Data</title>
      <link>https://temuller.github.io/post/fitting_data/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://temuller.github.io/post/fitting_data/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

import scipy
import lmfit
import emcee
#import pymc3
import pystan
import iminuit
from iminuit.util import describe, make_func_code

from keras.layers import Dense, Activation
from keras.models import Sequential

from multiprocessing import Pool
from chainconsumer import ChainConsumer

sns.set(context=&#39;talk&#39;, style=&#39;white&#39;)
%config InlineBackend.figure_format = &#39;retina&#39;

np.random.seed(32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example was taken from the &lt;code&gt;emcee&lt;/code&gt; documentation.&lt;/p&gt;
&lt;p&gt;To avoid correlation between parameters in this case, one would need to shift the x-axis by the mean value, but I will ommit that in here for simplicity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Choose the &amp;quot;true&amp;quot; parameters.
m_true = -0.9594
b_true = 4.294
f_true = 0.534

# Generate some synthetic data from the model.
N = 50
x = np.sort(10 * np.random.rand(N))
yerr = 0.1 + 0.5 * np.random.rand(N)
y = m_true * x + b_true
y += np.abs(f_true * y) * np.random.randn(N)
y += yerr * np.random.randn(N)

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
x0 = np.linspace(0, 10, 500)
plt.plot(x0, m_true * x0 + b_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scipy---minimize&#34;&gt;scipy - minimize&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_likelihood(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr**2
    return np.sum((y - model)**2 / sigma2)

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
results = scipy.optimize.minimize(log_likelihood, p0, args=(x, y, yerr))

m_pred, b_pred = results.x

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 (m_true = -0.9594)
b = 3.792 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---curve_fit&#34;&gt;scipy - curve_fit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def function(x, m, b):
    model = m*x + b
    return model

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov = scipy.optimize.curve_fit(function, x, y, p0=p0, sigma=yerr)

m_pred, b_pred = pfit
m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0647 (m_true = -0.9594)
b = 3.792 +/- 0.344 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---leastsq&#34;&gt;scipy - leastsq&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    return (model - y)/yerr

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov, infodict, errmsg, success = scipy.optimize.leastsq(residual_function, p0, 
                                                               args=(x, y, yerr), 
                                                               full_output=1)

m_pred, b_pred = pfit
try:
    m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])
except:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lmfit&#34;&gt;lmfit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(params, x, y, yerr):
    m, b = params[&#39;m&#39;].value, params[&#39;b&#39;].value
    model = m*x + b
    return ((model - y)/yerr)**2

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
params = lmfit.Parameters()
params.add(&#39;m&#39;, value=p0[0])
params.add(&#39;b&#39;, value=p0[1])
results = lmfit.minimizer.minimize(residual_function, params, args=(x, y, yerr)
                                   , method=&#39;lbfgsb&#39;)

m_pred, b_pred = results.params[&#39;m&#39;].value, results.params[&#39;b&#39;].value
m_std, b_std = results.params[&#39;m&#39;].stderr, results.params[&#39;b&#39;].stderr
if m_std is None and b_std is None:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8605 +/- inf (m_true = -0.9594)
b = 4.010 +/- inf (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iminuit&#34;&gt;iminuit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from iminuit import Minuit

def line(m, b, x, y):
    return m*x + b

def residual_function(m, b):
    #b, m = theta
    model = line(m, b, x, y)
    return np.sum(((model - y)/yerr)**2)

minu = Minuit(residual_function)

minu.migrad()  # run optimiser
minu.hesse()   # run covariance estimator
minu.minos()  # run minos estimator

m_pred, b_pred = minu.values.values()
m_std, b_std = minu.errors.values()
m_std_max, b_std_max, m_std_min, b_std_min = minu.merrors.values()

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(&#39;Hesse&#39;)
print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
print(&#39;Minos&#39;)
print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- ({b_std_min:.3f}, {b_std_max:.3f}) (b_true = {b_true})&#39;)

minu.draw_mncontour(&#39;m&#39;, &#39;b&#39;, nsigma=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_13_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hesse
m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
Minos
m = -0.8139 +/- (-0.0127, 0.0127) (m_true = -0.9594)
b = 3.792 +/- (-0.068, 0.068) (b_true = 4.294)





&amp;lt;matplotlib.contour.ContourSet at 0x7fd0b817a208&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_13_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;mcmc-inference&#34;&gt;MCMC inference&lt;/h1&gt;
&lt;p&gt;There are a couple of packages for plotting the samples with these methods. One is &lt;code&gt;corner&lt;/code&gt;, which is well known by most people I would say, and the other one, which I actually prefer and use here, is &lt;code&gt;chainconsumer&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;emcee&#34;&gt;emcee&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_like(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr ** 2
    return -0.5 * np.sum((y - model)**2/sigma2 + np.log(sigma2))

def log_prior(theta):
    m, b = theta
    if -5.0 &amp;lt; m &amp;lt; 0.5 and 0.0 &amp;lt; b &amp;lt; 10.0:
        return 0.0
    return -np.inf

def log_probability(theta, x, y, yerr):
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    return lp + log_like(theta, x, y, yerr)

pos = np.array([m_true, b_true]) + 1e-4*np.random.randn(32, 2)
nwalkers, ndim = pos.shape

with Pool() as pool:
    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, 
                                    args=(x, y, yerr), pool=pool)
    sampler.run_mcmc(pos, 4000, progress=True)

samples = sampler.chain[:, 1000:, :].reshape((-1, ndim))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100%|██████████| 4000/4000 [00:13&amp;lt;00:00, 288.01it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cc = ChainConsumer()
cc.add_chain(samples, parameters=[&#39;m&#39;, &#39;b&#39;])

# plot chains
fig = cc.plotter.plot_walks(truth={&amp;quot;m&amp;quot;: m_true, &amp;quot;b&amp;quot;: b_true}, convolve=100)
plt.show()

# plot contours
fig = cc.plotter.plot(figsize=float(ndim), truth={&amp;quot;m&amp;quot;: m_true, &amp;quot;b&amp;quot;: b_true})
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_17_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m_mcmc = np.percentile(samples[:, 0], [16, 50, 84])
b_mcmc = np.percentile(samples[:, 1], [16, 50, 84])
m_pred, b_pred = m_mcmc[1], b_mcmc[1]
m_std_min, m_std_max = np.diff(m_mcmc)
b_std_min, b_std_max = np.diff(b_mcmc)

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.4f} +/- ({b_std_min:.4f}, {b_std_max:.4f}) (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8131 +/- (0.0127, 0.0124) (m_true = -0.9594)
b = 3.7896 +/- (0.0679, 0.0674) (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pystan&#34;&gt;pystan&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = &amp;quot;&amp;quot;&amp;quot;
        data {
            int&amp;lt;lower=0&amp;gt; N;
            vector[N] x;
            vector[N] y;
        }
        parameters {
            real m;
            real b;
            real&amp;lt;lower=0&amp;gt; sigma;
        }
        model {
            y ~ normal(b + m*x, sigma);
        }
        &amp;quot;&amp;quot;&amp;quot;

data = {&#39;N&#39;: len(x), &#39;x&#39;: x, &#39;y&#39;: y}

# Compile the model
sm = pystan.StanModel(model_code=model)

# Train the model and generate samples
fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1defb22038d84b88c73c6495096e3e42 NOW.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;summary_dict = fit.summary()
df = pd.DataFrame(summary_dict[&#39;summary&#39;], 
                  columns=summary_dict[&#39;summary_colnames&#39;], 
                  index=summary_dict[&#39;summary_rownames&#39;])

m_pred, b_pred = df[&#39;mean&#39;][&#39;m&#39;], df[&#39;mean&#39;][&#39;b&#39;]
m_std, b_std = df[&#39;sd&#39;][&#39;m&#39;], df[&#39;sd&#39;][&#39;b&#39;]

# Extracting traces
m_trace = fit[&#39;m&#39;]
b_trace = fit[&#39;b&#39;]
sigma = fit[&#39;sigma&#39;]
lp = fit[&#39;lp__&#39;]

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.7944 +/- 0.0828 (m_true = -0.9594)
b = 3.595 +/- 0.492 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cc = ChainConsumer()
cc.add_chain(np.array([m_trace, b_trace]).T, parameters=[&#39;m&#39;, &#39;b&#39;])

# plot chains
fig = cc.plotter.plot_walks(truth={&amp;quot;m&amp;quot;: m_true, &amp;quot;b&amp;quot;: b_true}, convolve=100)
plt.show()

# plot contours
fig = cc.plotter.plot(figsize=float(ndim), truth={&amp;quot;m&amp;quot;: m_true, &amp;quot;b&amp;quot;: b_true})
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_22_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pymc3&#34;&gt;pymc3&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basic_model = pymc3.Model()

with basic_model:
    
    p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
    # Priors for unknown model parameters
    m = pymc3.Normal(&#39;m&#39;, mu=p0[0], sigma=2)
    b = pymc3.Normal(&#39;b&#39;, mu=p0[1], sigma=5)
    sigma = pymc3.HalfNormal(&#39;sigma&#39;, sigma=1)

    # Expected value of outcome
    model =m*x + b

    # Likelihood (sampling distribution) of observations
    Y_obs = pymc3.Normal(&#39;Y_obs&#39;, mu=model, sigma=sigma, observed=y)
    
map_estimate = pymc3.find_MAP(model=basic_model)

with basic_model:
   # instantiate sampler
    step = pymc3.Slice()

    # draw 5000 posterior samples
    trace = pymc3.sample(5000, step=step)

pymc3.traceplot(trace);
pymc3.summary(trace).round(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-packages&#34;&gt;Other packages&lt;/h2&gt;
&lt;p&gt;There are other packages for performing MCMC inference like: &lt;code&gt;Pyro/NumPyro&lt;/code&gt;, &lt;code&gt;mici&lt;/code&gt;, &lt;code&gt;TensorFlow Probability&lt;/code&gt; and &lt;code&gt;Sampyl&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;neural-networks-regression&#34;&gt;Neural Networks regression&lt;/h1&gt;
&lt;p&gt;The NN will fit the data without a given model. A proper fit would require training sets, testing sets and cross validation, but here only the most basic implementation is shown.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# NN model
model = Sequential()
model.add(Dense(32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 1))

# Compiling the ANN
model.compile(optimizer = &#39;adam&#39;, loss = &#39;mean_squared_error&#39;)

model.fit(x[:, None], y, batch_size = 10, epochs = 100, verbose=0)

y_pred = model.predict(x[:, None])
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
#plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.plot(x[:, None], y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
