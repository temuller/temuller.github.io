<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Tomás E. Müller Bravo</title>
    <link>https://temuller.github.io/post/</link>
      <atom:link href="https://temuller.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><copyright>© Tomás Müller `2020`</copyright><lastBuildDate>Wed, 16 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://temuller.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://temuller.github.io/post/</link>
    </image>
    
    <item>
      <title> Organising my first conference</title>
      <link>https://temuller.github.io/post/student_led_conference/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://temuller.github.io/post/student_led_conference/</guid>
      <description>&lt;p&gt;Every year 
&lt;a href=&#34;http://www.sepnet.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEPnet&lt;/a&gt; gives the opportunity to a group of postgraduate researchers (PGRs) to organise a two-days student-led conference hosted in Southampton. I first came to know about this when I applied for a talk at the Astronomy conference from 2019 (
&lt;a href=&#34;http://www.sepnet.ac.uk/call-for-abstracts-from-infinity-to-zero-the-history-of-the-universe-in-redshift-3-5-april-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From Infinity to Zero: the history of the Universe in redshift&lt;/a&gt;). These type of conferences are perfect for early career researchers as it is organised by student, for students, although a few academic speakers are also invited. You do not feel the same pressure as with the big international conferences where you find all the &amp;ldquo;big names&amp;rdquo; from your research field. In addition, you also get to know lots of people in the same career stages as you and share experiences.&lt;/p&gt;
&lt;p&gt;After the conference I attended was over, I got very interested in organising one of my own. Therefore, I talked to &lt;strong&gt;Elizabeth Swann&lt;/strong&gt;, the lead organiser of the 2019 meeting, to ask her for advise (this was really helpful!). I got very excited with the idea, so I decided to ask around in my Astronomy departement (University of Southampton) for fellow PGRs interested in organising a conference together and started writing a proposal. It happened that, at the same time, a group of PGRs from the University of Hertfordshire were writing one of their own. They kindly suggested to work together instead of compiting, so we started collaborating on a single proposal (afterall, isn&amp;rsquo;t this what research is all about?).&lt;/p&gt;
&lt;p&gt;Choosing the topic of the conference was relatively easy. As many big telescopes and surveys are coming in the near future, we thought it might be a good idea to focus on big data and machine learning, thus, the title of the conference: 
&lt;a href=&#34;https://sites.google.com/view/the-big-data-era-in-astronomy/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Big Data Era in Astronomy&lt;/a&gt;. Finding academic speakers wasn&amp;rsquo;t too hard either. As several of us in the 
&lt;a href=&#34;https://sites.google.com/view/the-big-data-era-in-astronomy/committee&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;organising committee&lt;/a&gt; work on different fields, we quickly came up with a list of candidates. &lt;strong&gt;Cristobel Soares-Smith&lt;/strong&gt; was in charge of most of the logistics (including funding) and she was also very helpful with her advice, so we mainly had to focus on the structure, science and social events (for example, a conference dinner) of the meeting. Everything was going great as the start of the conference was approaching, however, everything changed when the COVID-19 pandemic struck.&lt;/p&gt;
&lt;p&gt;Many things were quite uncertain at that time. We didn&amp;rsquo;t know how long and how much this pandemic would affect everyone, so we had to postpone the conference. As time started passing by, we grew impasient. We didn&amp;rsquo;t know if we were going to have the opportunity to host the meeting or if we would have to cancel it. Eventually, we decided to do what many other conferences, schools and workshops were doing, choose a new date and go virtual!&lt;/p&gt;
&lt;p&gt;This was full of challenges. We didn&amp;rsquo;t have to worry about funding, conference dinner and other things, but we did have to think about the proper platforms to host our virtual conference on. Thankfully, all Universities in SEPnet have access to &lt;strong&gt;Microsoft Teams&lt;/strong&gt;, thus, we chose it as our platform for hosting the talks. In addition, &lt;strong&gt;Slack&lt;/strong&gt; is widely used in academic environments as it is perfect for asynchronous discussions, questions, announcements, etc. We also decided to use &lt;strong&gt;Slido&lt;/strong&gt; for the questions at the end of each talk, which was quite new to all of us (I got to know about Slido during an 
&lt;a href=&#34;https://www.eso.org/public/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESO&lt;/a&gt; conference in June).&lt;/p&gt;
&lt;p&gt;Unfortunately, as things were still uncertain close to the new date of the conference in September, many of the students were unable to attend. Therefore, we had to shorten the length of the conference from two to one day, and cut the number of sessions by half. However, the conference turned out better than expected. From the feedback of the participants we learnt that the length of the conference (including the length of talks, breaks, etc.) was of their liking. Furthermore, the platforms used (Microsoft Teams, Slack, Slido) were really useful and the attendees found that the conference was worth assisting. Nonetheless, most of them, given the opportunity to choose, would prefer an in-person meeting instead of a virtual one.&lt;/p&gt;
&lt;p&gt;Networking and interacting with other people online is not as easy as in person. It is hard to have spontaneous and/or informal encounters. So, from my point of view, hosting a virtual conference has many advantages (for example, it is cheaper, easier to organise, no need to travel, etc.) and is a great option given the current circumstances, but it would always lack the face-to-face interaction which is a big part of these events. However, it is clear that the pandemic has opened a new window to future conferences as virtual meeting are becoming more common and reliable than in the past.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fitting Data - Basic implementation of Python packages</title>
      <link>https://temuller.github.io/post/fitting_data/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://temuller.github.io/post/fitting_data/</guid>
      <description>&lt;p&gt;In this notebook I show some basic implementation of different Python packages for data fitting. The idea is to learn the different options there are out there so the reader can then study them in more detail if needed.
This notebook can be opened on 
&lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;google colab&lt;/a&gt; or 
&lt;a href=&#34;https://mybinder.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binder&lt;/a&gt;. If for some reason there is a package missing, you will need to manually install it by running &lt;code&gt;!pip install &amp;lt;package&amp;gt;&lt;/code&gt; in a cell.&lt;/p&gt;
&lt;p&gt;To open this notebook on google colab, click in the following icon: 
&lt;a href=&#34;https://colab.research.google.com/github/temuller/personal_website/blob/master/content/post/fitting_data/basic_fitting_routines.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To open this notebook on binder, click in the following icon: 
&lt;a href=&#34;https://mybinder.org/v2/gh/temuller/personal_website/master?filepath=content%2Fpost%2Ffitting_data%2Fbasic_fitting_routines.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

import scipy
import lmfit
import emcee
#import pymc3  # borken installation?
import pystan
import iminuit
from iminuit.util import describe, make_func_code

from keras.layers import Dense, Activation
from keras.models import Sequential

from multiprocessing import Pool
import corner

sns.set(context=&#39;talk&#39;, style=&#39;white&#39;)
%config InlineBackend.figure_format = &#39;retina&#39;

np.random.seed(32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example, which represents data taken from a line, was taken from the &lt;code&gt;emcee&lt;/code&gt; documentation.&lt;/p&gt;
&lt;p&gt;To avoid correlation between parameters in this case, one would need to shift the x-axis by the mean value, but I will ommit that in here for simplicity. I will only show how to implement the different packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Choose the &amp;quot;true&amp;quot; parameters.
m_true = -0.9594
b_true = 4.294
f_true = 0.534

# Generate some synthetic data from the model.
N = 50
x = np.sort(10 * np.random.rand(N))
yerr = 0.1 + 0.5 * np.random.rand(N)
y = m_true * x + b_true
y += np.abs(f_true * y) * np.random.randn(N)
y += yerr * np.random.randn(N)

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
x0 = np.linspace(0, 10, 500)
plt.plot(x0, m_true * x0 + b_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scipy---minimize&#34;&gt;scipy - minimize&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_likelihood(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr**2
    return np.sum((y - model)**2 / sigma2)

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
results = scipy.optimize.minimize(log_likelihood, p0, args=(x, y, yerr))

m_pred, b_pred = results.x

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 (m_true = -0.9594)
b = 3.792 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---curve_fit&#34;&gt;scipy - curve_fit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def function(x, m, b):
    model = m*x + b
    return model

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov = scipy.optimize.curve_fit(function, x, y, p0=p0, sigma=yerr)

m_pred, b_pred = pfit
m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0647 (m_true = -0.9594)
b = 3.792 +/- 0.344 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---leastsq&#34;&gt;scipy - leastsq&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    return (model - y)/yerr

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov, infodict, errmsg, success = scipy.optimize.leastsq(residual_function, p0, 
                                                               args=(x, y, yerr), 
                                                               full_output=1)

m_pred, b_pred = pfit
try:
    m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])
except:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lmfit&#34;&gt;lmfit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(params, x, y, yerr):
    m, b = params[&#39;m&#39;].value, params[&#39;b&#39;].value
    model = m*x + b
    return ((model - y)/yerr)**2

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
params = lmfit.Parameters()
params.add(&#39;m&#39;, value=p0[0])
params.add(&#39;b&#39;, value=p0[1])
results = lmfit.minimizer.minimize(residual_function, params, args=(x, y, yerr)
                                   , method=&#39;lbfgsb&#39;)

m_pred, b_pred = results.params[&#39;m&#39;].value, results.params[&#39;b&#39;].value
m_std, b_std = results.params[&#39;m&#39;].stderr, results.params[&#39;b&#39;].stderr
if m_std is None and b_std is None:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8605 +/- inf (m_true = -0.9594)
b = 4.010 +/- inf (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iminuit&#34;&gt;iminuit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from iminuit import Minuit

def line(m, b, x, y):
    return m*x + b

def residual_function(m, b):
    #b, m = theta
    model = line(m, b, x, y)
    return np.sum(((model - y)/yerr)**2)

minu = Minuit(residual_function)

minu.migrad()  # run optimiser
minu.hesse()   # run covariance estimator
minu.minos()  # run minos estimator

m_pred, b_pred = minu.values.values()
m_std, b_std = minu.errors.values()
m_std_max, b_std_max, m_std_min, b_std_min = minu.merrors.values()

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(&#39;Hesse&#39;)
print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
print(&#39;Minos&#39;)
print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- ({b_std_min:.3f}, {b_std_max:.3f}) (b_true = {b_true})&#39;)

minu.draw_mncontour(&#39;m&#39;, &#39;b&#39;, nsigma=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m does not have initial value. Assume 0.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m is floating but does not have initial step size. Assume 1.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b does not have initial value. Assume 0.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b is floating but does not have initial step size. Assume 1.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: errordef is not given. Default to 1.
  # This is added back by InteractiveShellApp.init_path()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_14_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hesse
m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
Minos
m = -0.8139 +/- (-0.0127, 0.0127) (m_true = -0.9594)
b = 3.792 +/- (-0.068, 0.068) (b_true = 4.294)





&amp;lt;matplotlib.contour.ContourSet at 0x7fd0b817a208&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_14_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;mcmc-inference&#34;&gt;MCMC inference&lt;/h1&gt;
&lt;p&gt;There are a couple of packages for plotting the samples with these methods. One is &lt;code&gt;corner&lt;/code&gt;, which is well known by most people I would say, and the other one, which I actually prefer and use here, is &lt;code&gt;chainconsumer&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;emcee&#34;&gt;emcee&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_like(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr ** 2
    return -0.5 * np.sum((y - model)**2/sigma2 + np.log(sigma2))

def log_prior(theta):
    m, b = theta
    if -5.0 &amp;lt; m &amp;lt; 0.5 and 0.0 &amp;lt; b &amp;lt; 10.0:
        return 0.0
    return -np.inf

def log_probability(theta, x, y, yerr):
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    return lp + log_like(theta, x, y, yerr)

pos = np.array([m_true, b_true]) + 1e-4*np.random.randn(32, 2)
nwalkers, ndim = pos.shape

with Pool() as pool:
    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, 
                                    args=(x, y, yerr), pool=pool)
    sampler.run_mcmc(pos, 4000, progress=True)

samples = sampler.chain[:, 1000:, :].reshape((-1, ndim))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100%|██████████| 4000/4000 [00:54&amp;lt;00:00, 73.10it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True)
labels = [&amp;quot;m&amp;quot;, &amp;quot;b&amp;quot;]

for i in range(ndim):
    ax = axes[i]
    ax.plot(samples[:, i], &amp;quot;k&amp;quot;, alpha=0.6)
    ax.set_xlim(0, len(samples))
    ax.set_ylabel(labels[i])
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel(&amp;quot;step number&amp;quot;);

fig = corner.corner(
    samples, labels=labels, truths=[m_true, b_true]
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_18_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m_mcmc = np.percentile(samples[:, 0], [16, 50, 84])
b_mcmc = np.percentile(samples[:, 1], [16, 50, 84])
m_pred, b_pred = m_mcmc[1], b_mcmc[1]
m_std_min, m_std_max = np.diff(m_mcmc)
b_std_min, b_std_max = np.diff(b_mcmc)

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.4f} +/- ({b_std_min:.4f}, {b_std_max:.4f}) (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8131 +/- (0.0127, 0.0124) (m_true = -0.9594)
b = 3.7896 +/- (0.0679, 0.0674) (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pystan&#34;&gt;pystan&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = &amp;quot;&amp;quot;&amp;quot;
        data {
            int&amp;lt;lower=0&amp;gt; N;
            vector[N] x;
            vector[N] y;
        }
        parameters {
            real m;
            real b;
            real&amp;lt;lower=0&amp;gt; sigma;
        }
        model {
            y ~ normal(b + m*x, sigma);
        }
        &amp;quot;&amp;quot;&amp;quot;

data = {&#39;N&#39;: len(x), &#39;x&#39;: x, &#39;y&#39;: y}

# Compile the model
sm = pystan.StanModel(model_code=model)

# Train the model and generate samples
fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1defb22038d84b88c73c6495096e3e42 NOW.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;summary_dict = fit.summary()
df = pd.DataFrame(summary_dict[&#39;summary&#39;], 
                  columns=summary_dict[&#39;summary_colnames&#39;], 
                  index=summary_dict[&#39;summary_rownames&#39;])

m_pred, b_pred = df[&#39;mean&#39;][&#39;m&#39;], df[&#39;mean&#39;][&#39;b&#39;]
m_std, b_std = df[&#39;sd&#39;][&#39;m&#39;], df[&#39;sd&#39;][&#39;b&#39;]

# Extracting traces
m_trace = fit[&#39;m&#39;]
b_trace = fit[&#39;b&#39;]
sigma = fit[&#39;sigma&#39;]
lp = fit[&#39;lp__&#39;]

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.7944 +/- 0.0828 (m_true = -0.9594)
b = 3.595 +/- 0.492 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;samples = np.array([m_trace, b_trace]).T
                   
fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True)
labels = [&amp;quot;m&amp;quot;, &amp;quot;b&amp;quot;]

for i in range(ndim):
    ax = axes[i]
    ax.plot(samples[:, i], &amp;quot;k&amp;quot;, alpha=0.6)
    ax.set_xlim(0, len(samples))
    ax.set_ylabel(labels[i])
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel(&amp;quot;step number&amp;quot;);

fig = corner.corner(
    samples, labels=labels, truths=[m_true, b_true]
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_23_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pymc3&#34;&gt;pymc3&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basic_model = pymc3.Model()

with basic_model:
    
    p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
    # Priors for unknown model parameters
    m = pymc3.Normal(&#39;m&#39;, mu=p0[0], sigma=2)
    b = pymc3.Normal(&#39;b&#39;, mu=p0[1], sigma=5)
    sigma = pymc3.HalfNormal(&#39;sigma&#39;, sigma=1)

    # Expected value of outcome
    model =m*x + b

    # Likelihood (sampling distribution) of observations
    Y_obs = pymc3.Normal(&#39;Y_obs&#39;, mu=model, sigma=sigma, observed=y)
    
map_estimate = pymc3.find_MAP(model=basic_model)

with basic_model:
   # instantiate sampler
    step = pymc3.Slice()

    # draw 5000 posterior samples
    trace = pymc3.sample(5000, step=step)

pymc3.traceplot(trace);
pymc3.summary(trace).round(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-packages&#34;&gt;Other packages&lt;/h2&gt;
&lt;p&gt;There are other packages for performing MCMC inference like: &lt;code&gt;Pyro/NumPyro&lt;/code&gt;, &lt;code&gt;mici&lt;/code&gt;, &lt;code&gt;TensorFlow Probability&lt;/code&gt; and &lt;code&gt;Sampyl&lt;/code&gt; (I might be missing a couple though). Feel free to check those as well.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;artificial-neural-networks-ann-regression&#34;&gt;Artificial Neural Networks (ANN) regression&lt;/h1&gt;
&lt;p&gt;The ANN will fit the data without a given model. A proper fit would require training sets, testing sets and cross validation, but here only the most basic implementation is shown. There is much more you can do with ANN.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# NN model
model = Sequential()
model.add(Dense(32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 1))

# Compiling the ANN
model.compile(optimizer = &#39;adam&#39;, loss = &#39;mean_squared_error&#39;)

model.fit(x[:, None], y, batch_size = 10, epochs = 100, verbose=0)

y_pred = model.predict(x[:, None])
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
#plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.plot(x[:, None], y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title> Judging a book by its cover: estimating red supergiant masses from their surface abundance</title>
      <link>https://temuller.github.io/post/astrobites/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://temuller.github.io/post/astrobites/</guid>
      <description>&lt;p&gt;See below a guest post I wrote for 
&lt;a href=&#34;https://astrobites.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Astrobites&lt;/a&gt;. For the original one 
&lt;a href=&#34;https://astrobites.org/2019/01/28/judging-book-cover-red-supergiant/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; 
&lt;a href=&#34;https://arxiv.org/pdf/1811.04087.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The surface abundances of Red Supergiants at core-collapse&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ben Davies and Luc Dessart&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First Author&amp;rsquo;s Institution:&lt;/strong&gt; Astrophysics Research Institute, Liverpool John Moores University&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; accepted for publication in MNRAS&lt;/p&gt;
&lt;p&gt;Core-Collapse supernovae (CCSNe) are explosions coming from massive stars (above 8 solar masses) when they reach the end of their life. A 
&lt;a href=&#34;http://astronomy.swin.edu.au/cosmos/T/Type&amp;#43;II&amp;#43;Supernova&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Type II-P Supernova&lt;/a&gt; (SN II-P)  is a common type of CCSN which shows a &amp;ldquo;plateau&amp;rdquo; in its light curve, driven by a Hydrogen-rich envelope. Observations of the explosion site of some of these objects have shown that the progenitors of these explosions are 
&lt;a href=&#34;https://aasnova.org/2018/01/03/astrophysics-of-red-supergiants/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Red Supergiant&lt;/a&gt; (RSG) stars. From these data several predictions can be made by comparing with stellar evolution models. One test that can be made is to determine the initial mass of the progenitor (the mass at the Main-Sequence), which can be done by comparing the luminosity of the progenitor with model predictions. Another way is to measure the mass of the Hydrogen-rich envelope by modelling the light curve and making some assumptions about the core mass. A different way is to measure 
&lt;a href=&#34;https://astrobites.org/guides/spectroscopy-and-spectral-lines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spectral lines&lt;/a&gt; of some elements, like Oxygen, at late time (more than 120 days after the explosion, approximately), which correlates with the initial mass as shown by some models.&lt;/p&gt;
&lt;p&gt;Unfortunately, these methods do not agree in general, so a proper estimation of the initial mass of the progenitor of a SN II-P can not be made. Having this in mind, the authors of the article proposed a different way of estimating the initial mass by measuring the surface composition (or surface abundance) of the progenitor star at early epochs (less than 1 day approximately). There are two main reasons for this: firstly, at this stage some spectral features are easy to identify, and secondly, the surface abundance is not expected to suffer from explosive mixing at early time, which erases any link to the progenitor mass.&lt;/p&gt;
&lt;p&gt;To test this, the authors make use of the 
&lt;a href=&#34;http://mesa.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MESA&lt;/a&gt; code to study sets of evolutionary models. The evolution across the 
&lt;a href=&#34;http://astronomy.swin.edu.au/cosmos/H/Hertzsprung-Russell&amp;#43;Diagram&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hertzsprung-Russell&lt;/a&gt; (H-R) diagram is key to understand the different processes and phases a star goes through, so the authors use MESA to evolve stars with different initial masses to study this. As it can be seen in Figure 1, less massive stars cross the H-R diagram more rapidly than more massive star, which means that more massive stars have more time to dredge up material from the inner layers into the surface (mixing the abundances) before the end of the RSG phase. Another factor that has to be taken into account is the mass loss. In general, more massive star loose more mass than the least massive ones, so their outer envelopes are thinner compared to the total size of the star, hence their surface abundances suffer from more mixing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Fig1.png&#34; alt=&#34;png&#34;&gt;
&lt;em&gt;Fig. 1: H-R diagram for stars with different initial masses as labelled. The circles mark evenly-spaced time steps. The stars indicate the beginning of the RSG phase. More massive stars take longer to cross the H-R diagram than less massive stars.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since the surface abundance can not be directly measured, an indirect way of measurement was developed. The authors demonstrate that the surface abundance can be estimated by measuring the ratio between spectral lines, specifically the Carbon-to-Nitrogen ratio, a few hours after the supernova explosion. They simulate early time supernova spectra coming from stars with three different initial masses. The results of these simulated spectra are shown in Figure 2 for a 15 and 25 solar masses stars, for three epochs: 3.1, 12.0 and 24.0 hours after the explosion. One can clearly see the greater strength of Nitrogen lines in the higher mass model, and the greater strength of Carbon and Oxygen lines in the lower mass model. These same trends have been seen in the observed early-time spectra of SN II-P, however, the lack of data at these early stages is clear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Fig2.png&#34; alt=&#34;png&#34;&gt;
&lt;em&gt;Fig. 2: Early-time SN II-P spectra for progenitors of 15 (blue) and 25 (red) solar masses. From top to bottom, spectra of 3.1, 12.0 and 24.0 hours after explosion. It can be seen the difference in the strengths of lines for the two progenitors.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Like the other methods mentioned above, the surface abundance method suffers from several uncertainties, but no further comparison is given. Finally, the author estimate that within the next decade several early time spectra of SN II-P should be available, which would help decreasing the uncertainties for a better estimation of the initial masses of their progenitors.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
