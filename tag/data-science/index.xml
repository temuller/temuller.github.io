<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data science | Tomás E. Müller Bravo</title>
    <link>https://temuller.github.io/tag/data-science/</link>
      <atom:link href="https://temuller.github.io/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>data science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><copyright>© Tomás Müller `2021`</copyright><lastBuildDate>Fri, 17 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://temuller.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>data science</title>
      <link>https://temuller.github.io/tag/data-science/</link>
    </image>
    
    <item>
      <title>Fitting Data - Basic implementation of Python packages</title>
      <link>https://temuller.github.io/post/fitting_data/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://temuller.github.io/post/fitting_data/</guid>
      <description>&lt;p&gt;In this notebook I show some basic implementation of different Python packages for data fitting. The idea is to learn the different options there are out there so the reader can then study them in more detail if needed.
This notebook can be opened on 
&lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;google colab&lt;/a&gt; or 
&lt;a href=&#34;https://mybinder.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binder&lt;/a&gt;. If for some reason there is a package missing, you will need to manually install it by running &lt;code&gt;!pip install &amp;lt;package&amp;gt;&lt;/code&gt; in a cell.&lt;/p&gt;
&lt;p&gt;To open this notebook on google colab, click in the following icon: 
&lt;a href=&#34;https://colab.research.google.com/github/temuller/personal_website/blob/master/content/post/fitting_data/basic_fitting_routines.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To open this notebook on binder, click in the following icon: 
&lt;a href=&#34;https://mybinder.org/v2/gh/temuller/personal_website/master?filepath=content%2Fpost%2Ffitting_data%2Fbasic_fitting_routines.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

import scipy
import lmfit
import emcee
#import pymc3  # borken installation?
import pystan
import iminuit
from iminuit.util import describe, make_func_code

from keras.layers import Dense, Activation
from keras.models import Sequential

from multiprocessing import Pool
import corner

sns.set(context=&#39;talk&#39;, style=&#39;white&#39;)
%config InlineBackend.figure_format = &#39;retina&#39;

np.random.seed(32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example, which represents data taken from a line, was taken from the &lt;code&gt;emcee&lt;/code&gt; documentation.&lt;/p&gt;
&lt;p&gt;To avoid correlation between parameters in this case, one would need to shift the x-axis by the mean value, but I will ommit that in here for simplicity. I will only show how to implement the different packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Choose the &amp;quot;true&amp;quot; parameters.
m_true = -0.9594
b_true = 4.294
f_true = 0.534

# Generate some synthetic data from the model.
N = 50
x = np.sort(10 * np.random.rand(N))
yerr = 0.1 + 0.5 * np.random.rand(N)
y = m_true * x + b_true
y += np.abs(f_true * y) * np.random.randn(N)
y += yerr * np.random.randn(N)

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
x0 = np.linspace(0, 10, 500)
plt.plot(x0, m_true * x0 + b_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_4_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scipy---minimize&#34;&gt;scipy - minimize&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_likelihood(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr**2
    return np.sum((y - model)**2 / sigma2)

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
results = scipy.optimize.minimize(log_likelihood, p0, args=(x, y, yerr))

m_pred, b_pred = results.x

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 (m_true = -0.9594)
b = 3.792 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---curve_fit&#34;&gt;scipy - curve_fit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def function(x, m, b):
    model = m*x + b
    return model

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov = scipy.optimize.curve_fit(function, x, y, p0=p0, sigma=yerr)

m_pred, b_pred = pfit
m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0647 (m_true = -0.9594)
b = 3.792 +/- 0.344 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;scipy---leastsq&#34;&gt;scipy - leastsq&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    return (model - y)/yerr

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
pfit, pcov, infodict, errmsg, success = scipy.optimize.leastsq(residual_function, p0, 
                                                               args=(x, y, yerr), 
                                                               full_output=1)

m_pred, b_pred = pfit
try:
    m_std, b_std = np.sqrt(pcov[0, 0]), np.sqrt(pcov[1, 1])
except:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lmfit&#34;&gt;lmfit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def residual_function(params, x, y, yerr):
    m, b = params[&#39;m&#39;].value, params[&#39;b&#39;].value
    model = m*x + b
    return ((model - y)/yerr)**2

p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
params = lmfit.Parameters()
params.add(&#39;m&#39;, value=p0[0])
params.add(&#39;b&#39;, value=p0[1])
results = lmfit.minimizer.minimize(residual_function, params, args=(x, y, yerr)
                                   , method=&#39;lbfgsb&#39;)

m_pred, b_pred = results.params[&#39;m&#39;].value, results.params[&#39;b&#39;].value
m_std, b_std = results.params[&#39;m&#39;].stderr, results.params[&#39;b&#39;].stderr
if m_std is None and b_std is None:
    m_std = b_std = np.inf

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8605 +/- inf (m_true = -0.9594)
b = 4.010 +/- inf (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iminuit&#34;&gt;iminuit&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from iminuit import Minuit

def line(m, b, x, y):
    return m*x + b

def residual_function(m, b):
    #b, m = theta
    model = line(m, b, x, y)
    return np.sum(((model - y)/yerr)**2)

minu = Minuit(residual_function)

minu.migrad()  # run optimiser
minu.hesse()   # run covariance estimator
minu.minos()  # run minos estimator

m_pred, b_pred = minu.values.values()
m_std, b_std = minu.errors.values()
m_std_max, b_std_max, m_std_min, b_std_min = minu.merrors.values()

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(&#39;Hesse&#39;)
print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
print(&#39;Minos&#39;)
print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- ({b_std_min:.3f}, {b_std_max:.3f}) (b_true = {b_true})&#39;)

minu.draw_mncontour(&#39;m&#39;, &#39;b&#39;, nsigma=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m does not have initial value. Assume 0.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter m is floating but does not have initial step size. Assume 1.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b does not have initial value. Assume 0.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: Parameter b is floating but does not have initial step size. Assume 1.
  # This is added back by InteractiveShellApp.init_path()
/home/tomas/anaconda3/envs/pisco/lib/python3.6/site-packages/ipykernel_launcher.py:11: InitialParamWarning: errordef is not given. Default to 1.
  # This is added back by InteractiveShellApp.init_path()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_14_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hesse
m = -0.8139 +/- 0.0127 (m_true = -0.9594)
b = 3.792 +/- 0.068 (b_true = 4.294)
Minos
m = -0.8139 +/- (-0.0127, 0.0127) (m_true = -0.9594)
b = 3.792 +/- (-0.068, 0.068) (b_true = 4.294)





&amp;lt;matplotlib.contour.ContourSet at 0x7fd0b817a208&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_14_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;mcmc-inference&#34;&gt;MCMC inference&lt;/h1&gt;
&lt;p&gt;There are a couple of packages for plotting the samples with these methods. One is &lt;code&gt;corner&lt;/code&gt;, which is well known by most people I would say, and the other one, which I actually prefer and use here, is &lt;code&gt;chainconsumer&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;emcee&#34;&gt;emcee&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_like(theta, x, y, yerr):
    m, b = theta
    model = m*x + b
    sigma2 = yerr ** 2
    return -0.5 * np.sum((y - model)**2/sigma2 + np.log(sigma2))

def log_prior(theta):
    m, b = theta
    if -5.0 &amp;lt; m &amp;lt; 0.5 and 0.0 &amp;lt; b &amp;lt; 10.0:
        return 0.0
    return -np.inf

def log_probability(theta, x, y, yerr):
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    return lp + log_like(theta, x, y, yerr)

pos = np.array([m_true, b_true]) + 1e-4*np.random.randn(32, 2)
nwalkers, ndim = pos.shape

with Pool() as pool:
    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, 
                                    args=(x, y, yerr), pool=pool)
    sampler.run_mcmc(pos, 4000, progress=True)

samples = sampler.chain[:, 1000:, :].reshape((-1, ndim))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100%|██████████| 4000/4000 [00:54&amp;lt;00:00, 73.10it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True)
labels = [&amp;quot;m&amp;quot;, &amp;quot;b&amp;quot;]

for i in range(ndim):
    ax = axes[i]
    ax.plot(samples[:, i], &amp;quot;k&amp;quot;, alpha=0.6)
    ax.set_xlim(0, len(samples))
    ax.set_ylabel(labels[i])
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel(&amp;quot;step number&amp;quot;);

fig = corner.corner(
    samples, labels=labels, truths=[m_true, b_true]
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_18_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m_mcmc = np.percentile(samples[:, 0], [16, 50, 84])
b_mcmc = np.percentile(samples[:, 1], [16, 50, 84])
m_pred, b_pred = m_mcmc[1], b_mcmc[1]
m_std_min, m_std_max = np.diff(m_mcmc)
b_std_min, b_std_max = np.diff(b_mcmc)

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- ({m_std_min:.4f}, {m_std_max:.4f}) (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.4f} +/- ({b_std_min:.4f}, {b_std_max:.4f}) (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.8131 +/- (0.0127, 0.0124) (m_true = -0.9594)
b = 3.7896 +/- (0.0679, 0.0674) (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pystan&#34;&gt;pystan&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = &amp;quot;&amp;quot;&amp;quot;
        data {
            int&amp;lt;lower=0&amp;gt; N;
            vector[N] x;
            vector[N] y;
        }
        parameters {
            real m;
            real b;
            real&amp;lt;lower=0&amp;gt; sigma;
        }
        model {
            y ~ normal(b + m*x, sigma);
        }
        &amp;quot;&amp;quot;&amp;quot;

data = {&#39;N&#39;: len(x), &#39;x&#39;: x, &#39;y&#39;: y}

# Compile the model
sm = pystan.StanModel(model_code=model)

# Train the model and generate samples
fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1defb22038d84b88c73c6495096e3e42 NOW.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;summary_dict = fit.summary()
df = pd.DataFrame(summary_dict[&#39;summary&#39;], 
                  columns=summary_dict[&#39;summary_colnames&#39;], 
                  index=summary_dict[&#39;summary_rownames&#39;])

m_pred, b_pred = df[&#39;mean&#39;][&#39;m&#39;], df[&#39;mean&#39;][&#39;b&#39;]
m_std, b_std = df[&#39;sd&#39;][&#39;m&#39;], df[&#39;sd&#39;][&#39;b&#39;]

# Extracting traces
m_trace = fit[&#39;m&#39;]
b_trace = fit[&#39;b&#39;]
sigma = fit[&#39;sigma&#39;]
lp = fit[&#39;lp__&#39;]

y_pred = m_pred*x0 + b_pred
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()

print(f&#39;m = {m_pred:.4f} +/- {m_std:.4f} (m_true = {m_true})&#39;)
print(f&#39;b = {b_pred:.3f} +/- {b_std:.3f} (b_true = {b_true})&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m = -0.7944 +/- 0.0828 (m_true = -0.9594)
b = 3.595 +/- 0.492 (b_true = 4.294)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;samples = np.array([m_trace, b_trace]).T
                   
fig, axes = plt.subplots(ndim, figsize=(10, 5), sharex=True)
labels = [&amp;quot;m&amp;quot;, &amp;quot;b&amp;quot;]

for i in range(ndim):
    ax = axes[i]
    ax.plot(samples[:, i], &amp;quot;k&amp;quot;, alpha=0.6)
    ax.set_xlim(0, len(samples))
    ax.set_ylabel(labels[i])
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel(&amp;quot;step number&amp;quot;);

fig = corner.corner(
    samples, labels=labels, truths=[m_true, b_true]
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_23_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pymc3&#34;&gt;pymc3&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basic_model = pymc3.Model()

with basic_model:
    
    p0 = np.array([m_true, b_true]) + 0.1 * np.random.randn(2)
    # Priors for unknown model parameters
    m = pymc3.Normal(&#39;m&#39;, mu=p0[0], sigma=2)
    b = pymc3.Normal(&#39;b&#39;, mu=p0[1], sigma=5)
    sigma = pymc3.HalfNormal(&#39;sigma&#39;, sigma=1)

    # Expected value of outcome
    model =m*x + b

    # Likelihood (sampling distribution) of observations
    Y_obs = pymc3.Normal(&#39;Y_obs&#39;, mu=model, sigma=sigma, observed=y)
    
map_estimate = pymc3.find_MAP(model=basic_model)

with basic_model:
   # instantiate sampler
    step = pymc3.Slice()

    # draw 5000 posterior samples
    trace = pymc3.sample(5000, step=step)

pymc3.traceplot(trace);
pymc3.summary(trace).round(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-packages&#34;&gt;Other packages&lt;/h2&gt;
&lt;p&gt;There are other packages for performing MCMC inference like: &lt;code&gt;Pyro/NumPyro&lt;/code&gt;, &lt;code&gt;mici&lt;/code&gt;, &lt;code&gt;TensorFlow Probability&lt;/code&gt; and &lt;code&gt;Sampyl&lt;/code&gt; (I might be missing a couple though). Feel free to check those as well.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;artificial-neural-networks-ann-regression&#34;&gt;Artificial Neural Networks (ANN) regression&lt;/h1&gt;
&lt;p&gt;The ANN will fit the data without a given model. A proper fit would require training sets, testing sets and cross validation, but here only the most basic implementation is shown. There is much more you can do with ANN.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# NN model
model = Sequential()
model.add(Dense(32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 32, activation = &#39;relu&#39;))
model.add(Dense(units = 1))

# Compiling the ANN
model.compile(optimizer = &#39;adam&#39;, loss = &#39;mean_squared_error&#39;)

model.fit(x[:, None], y, batch_size = 10, epochs = 100, verbose=0)

y_pred = model.predict(x[:, None])
y_true = m_true*x0 + b_true

plt.errorbar(x, y, yerr=yerr, fmt=&amp;quot;.k&amp;quot;, capsize=0)
plt.plot(x0, y_true, &amp;quot;k&amp;quot;, alpha=0.3, lw=3, label=&amp;quot;truth&amp;quot;)
#plt.plot(x0, y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.plot(x[:, None], y_pred, &amp;quot;:k&amp;quot;, label=&amp;quot;fit&amp;quot;)
plt.legend(fontsize=14)
plt.xlim(0, 10)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;basic_fitting_routines_files/basic_fitting_routines_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
